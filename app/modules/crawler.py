"""
DCì¸ì‚¬ì´ë“œ ë¯¸êµ­ì£¼ì‹ ê°¤ëŸ¬ë¦¬ í¬ë¡¤ëŸ¬
- ê°œë…ê¸€ ìœ„ì£¼ë¡œ ì œëª©/ë³¸ë¬¸/ì¶”ì²œìˆ˜/ëŒ“ê¸€ ìˆ˜ì§‘
- MongoDBì— Upsertë¡œ ì¤‘ë³µ ë°©ì§€
"""

import os
import time
import logging
from datetime import datetime
from typing import List, Dict, Optional
import requests
from bs4 import BeautifulSoup
from pymongo import MongoClient
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ìƒìˆ˜ ì„¤ì •
GALLERY_ID = "us_stocks"  # DCì¸ì‚¬ì´ë“œ ë¯¸êµ­ì£¼ì‹ ê°¤ëŸ¬ë¦¬ ID
BASE_URL = f"https://gall.dcinside.com/mgallery/board/lists"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
}


def get_mongo_client() -> MongoClient:
    """MongoDB í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    mongo_uri = os.getenv('MONGO_URI', 'mongodb://localhost:27017/')
    try:
        client = MongoClient(mongo_uri, serverSelectionTimeoutMS=5000)
        # ì—°ê²° í…ŒìŠ¤íŠ¸
        client.admin.command('ping')
        logger.info(f"âœ… MongoDB ì—°ê²° ì„±ê³µ: {mongo_uri}")
        return client
    except Exception as e:
        logger.error(f"âŒ MongoDB ì—°ê²° ì‹¤íŒ¨: {e}")
        raise


def get_post_list(page: int = 1, recommend_only: bool = True) -> List[Dict]:
    """
    ê°¤ëŸ¬ë¦¬ ê²Œì‹œê¸€ ëª©ë¡ í¬ë¡¤ë§
    
    Args:
        page: í˜ì´ì§€ ë²ˆí˜¸
        recommend_only: Trueë©´ ê°œë…ê¸€ë§Œ, Falseë©´ ì „ì²´ê¸€
    
    Returns:
        ê²Œì‹œê¸€ ì •ë³´ ë¦¬ìŠ¤íŠ¸ [{'post_id', 'title', 'author', 'date', 'views', 'recommend'}]
    """
    params = {
        'id': GALLERY_ID,
        'page': page,
    }
    
    # ê°œë…ê¸€ë§Œ ë³´ê¸° (ì¶”ì²œìˆ˜ ê¸°ì¤€)
    if recommend_only:
        params['recommend'] = '1'
    
    try:
        response = requests.get(BASE_URL, params=params, headers=HEADERS, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'lxml')
        posts = []
        
        # ê²Œì‹œê¸€ í…Œì´ë¸”ì—ì„œ tr.ub-content ì¶”ì¶œ
        rows = soup.select('tr.ub-content')
        
        for row in rows:
            try:
                # ê³µì§€ê¸€ ì œì™¸
                if 'notice' in row.get('class', []):
                    continue
                
                # ê²Œì‹œê¸€ ë²ˆí˜¸ ì¶”ì¶œ
                num_cell = row.select_one('td.gall_num')
                if not num_cell or not num_cell.text.strip().isdigit():
                    continue
                
                post_id = num_cell.text.strip()
                
                # ì œëª© ì¶”ì¶œ
                title_elem = row.select_one('td.gall_tit a')
                if not title_elem:
                    continue
                title = title_elem.text.strip()
                
                # ì‘ì„±ì
                author_elem = row.select_one('td.gall_writer')
                author = author_elem.get('data-nick', 'ìµëª…') if author_elem else 'ìµëª…'
                
                # ë‚ ì§œ
                date_elem = row.select_one('td.gall_date')
                date_str = date_elem.get('title', '') if date_elem else ''
                
                # ì¡°íšŒìˆ˜
                views_elem = row.select_one('td.gall_count')
                views = int(views_elem.text.strip()) if views_elem and views_elem.text.strip().isdigit() else 0
                
                # ì¶”ì²œìˆ˜
                recommend_elem = row.select_one('td.gall_recommend')
                recommend = int(recommend_elem.text.strip()) if recommend_elem and recommend_elem.text.strip().isdigit() else 0
                
                posts.append({
                    'post_id': post_id,
                    'title': title,
                    'author': author,
                    'date': date_str,
                    'views': views,
                    'recommend': recommend,
                })
                
            except Exception as e:
                logger.warning(f"ê²Œì‹œê¸€ íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"ğŸ“„ í˜ì´ì§€ {page}: {len(posts)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘")
        return posts
        
    except requests.RequestException as e:
        logger.error(f"âŒ í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {e}")
        return []


def get_post_detail(post_id: str) -> Optional[Dict]:
    """
    ê²Œì‹œê¸€ ë³¸ë¬¸ í¬ë¡¤ë§
    
    Args:
        post_id: ê²Œì‹œê¸€ ë²ˆí˜¸
    
    Returns:
        {'post_id', 'title', 'author', 'content', 'images', 'comments'}
    """
    url = f"https://gall.dcinside.com/mgallery/board/view"
    params = {
        'id': GALLERY_ID,
        'no': post_id,
    }
    
    try:
        response = requests.get(url, params=params, headers=HEADERS, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'lxml')
        
        # ì œëª©
        title_elem = soup.select_one('span.title_subject')
        title = title_elem.text.strip() if title_elem else ''
        
        # ì‘ì„±ì
        author_elem = soup.select_one('div.gall_writer')
        author = author_elem.get('data-nick', 'ìµëª…') if author_elem else 'ìµëª…'
        
        # ë³¸ë¬¸ (HTML íƒœê·¸ ì œê±°)
        content_elem = soup.select_one('div.write_div')
        if content_elem:
            # ì´ë¯¸ì§€ íƒœê·¸ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
            for img in content_elem.find_all('img'):
                img.decompose()
            content = content_elem.get_text(separator='\n', strip=True)
        else:
            content = ''
        
        # ì´ë¯¸ì§€ URL ìˆ˜ì§‘
        images = []
        if content_elem:
            for img in soup.select('div.write_div img'):
                src = img.get('src', '')
                if src and src.startswith('http'):
                    images.append(src)
        
        # ëŒ“ê¸€ ìˆ˜ì§‘ (ê°„ë‹¨íˆ ëŒ“ê¸€ ë‚´ìš©ë§Œ)
        comments = []
        comment_elems = soup.select('li.ub-content')
        for comment in comment_elems[:10]:  # ìµœëŒ€ 10ê°œë§Œ
            comment_text_elem = comment.select_one('p.usertxt')
            if comment_text_elem:
                comments.append(comment_text_elem.text.strip())
        
        logger.info(f"ğŸ“ ê²Œì‹œê¸€ {post_id} ë³¸ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ (ë³¸ë¬¸ {len(content)}ì, ëŒ“ê¸€ {len(comments)}ê°œ)")
        
        return {
            'post_id': post_id,
            'title': title,
            'author': author,
            'content': content,
            'images': images,
            'comments': comments,
        }
        
    except requests.RequestException as e:
        logger.error(f"âŒ ê²Œì‹œê¸€ {post_id} ìš”ì²­ ì‹¤íŒ¨: {e}")
        return None
    except Exception as e:
        logger.error(f"âŒ ê²Œì‹œê¸€ {post_id} íŒŒì‹± ì‹¤íŒ¨: {e}")
        return None


def save_to_mongodb(posts: List[Dict], db_name: str = None) -> int:
    """
    í¬ë¡¤ë§í•œ ê²Œì‹œê¸€ì„ MongoDBì— ì €ì¥ (Upsert)
    
    Args:
        posts: ì €ì¥í•  ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸
        db_name: ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„
    
    Returns:
        ì €ì¥ëœ ê²Œì‹œê¸€ ìˆ˜
    """
    if not posts:
        return 0
    
    db_name = db_name or os.getenv('MONGO_DB_NAME', 'shorts_factory')
    
    try:
        client = get_mongo_client()
        db = client[db_name]
        collection = db['posts']
        
        saved_count = 0
        for post in posts:
            # post_idë¥¼ ê¸°ì¤€ìœ¼ë¡œ Upsert
            post['crawled_at'] = datetime.now()
            result = collection.update_one(
                {'post_id': post['post_id']},
                {'$set': post},
                upsert=True
            )
            if result.upserted_id or result.modified_count > 0:
                saved_count += 1
        
        logger.info(f"ğŸ’¾ MongoDB ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ (ì „ì²´ {len(posts)}ê°œ)")
        return saved_count
        
    except Exception as e:
        logger.error(f"âŒ MongoDB ì €ì¥ ì‹¤íŒ¨: {e}")
        return 0


def crawl_gallery(pages: int = 3, delay: float = 2.0, save_to_db: bool = True) -> List[Dict]:
    """
    ê°¤ëŸ¬ë¦¬ í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜
    
    Args:
        pages: í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜
        delay: í˜ì´ì§€ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        save_to_db: MongoDB ì €ì¥ ì—¬ë¶€
    
    Returns:
        í¬ë¡¤ë§í•œ ì „ì²´ ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸
    """
    logger.info(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘: {pages}í˜ì´ì§€, ì§€ì—° {delay}ì´ˆ")
    
    all_posts = []
    
    # 1ë‹¨ê³„: ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘
    for page in range(1, pages + 1):
        posts = get_post_list(page=page, recommend_only=True)
        
        # 2ë‹¨ê³„: ê° ê²Œì‹œê¸€ ë³¸ë¬¸ ìˆ˜ì§‘
        for post in posts:
            time.sleep(delay)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
            
            detail = get_post_detail(post['post_id'])
            if detail:
                # ëª©ë¡ ì •ë³´ì™€ ë³¸ë¬¸ ì •ë³´ ë³‘í•©
                merged = {**post, **detail}
                all_posts.append(merged)
        
        time.sleep(delay)  # í˜ì´ì§€ ê°„ ëŒ€ê¸°
    
    logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ê²Œì‹œê¸€")
    
    # MongoDB ì €ì¥
    if save_to_db:
        save_to_mongodb(all_posts)
    
    return all_posts


if __name__ == '__main__':
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    crawl_gallery(pages=2, delay=1.5)

