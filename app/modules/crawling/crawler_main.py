"""
í¬ë¡¤ë§ ë©”ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°

DCì¸ì‚¬ì´ë“œ í¬ë¡¤ë§ + MongoDB ì €ì¥ì„ ì¡°ìœ¨í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
"""

import time
import logging
from typing import List, Dict
from dotenv import load_dotenv

# ë¶„ë¦¬ëœ ëª¨ë“ˆë“¤ import (ë™ì¼ íŒ¨í‚¤ì§€ ë‚´ ìƒëŒ€ ê²½ë¡œ)
from .dcinside.list_scraper import get_post_list
from .dcinside.detail_scraper import get_post_detail
from .manager.save_db import save_posts

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def crawl_gallery(pages: int = 3, delay: float = 2.0, save_to_db: bool = True) -> List[Dict]:
    """
    ê°¤ëŸ¬ë¦¬ í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜
    
    Args:
        pages: í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜
        delay: í˜ì´ì§€ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        save_to_db: MongoDB ì €ì¥ ì—¬ë¶€
    
    Returns:
        í¬ë¡¤ë§í•œ ì „ì²´ ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸
    """
    logger.info(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘: {pages}í˜ì´ì§€, ì§€ì—° {delay}ì´ˆ")
    
    all_posts = []
    
    # 1ë‹¨ê³„: ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘
    for page in range(1, pages + 1):
        posts = get_post_list(page=page, recommend_only=True)
        
        # 2ë‹¨ê³„: ê° ê²Œì‹œê¸€ ë³¸ë¬¸ ìˆ˜ì§‘
        for post in posts:
            time.sleep(delay)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
            
            detail = get_post_detail(post['post_id'])
            if detail:
                # ëª©ë¡ ì •ë³´ì™€ ë³¸ë¬¸ ì •ë³´ ë³‘í•©
                merged = {**post, **detail}
                all_posts.append(merged)
        
        time.sleep(delay)  # í˜ì´ì§€ ê°„ ëŒ€ê¸°
    
    logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ê²Œì‹œê¸€")
    
    # MongoDB ì €ì¥
    if save_to_db:
        save_posts(all_posts)
    
    return all_posts


if __name__ == '__main__':
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    crawl_gallery(pages=2, delay=1.5)
