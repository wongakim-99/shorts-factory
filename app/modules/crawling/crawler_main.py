"""
í¬ë¡¤ë§ ë©”ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°

DCì¸ì‚¬ì´ë“œ í¬ë¡¤ë§ + MongoDB ì €ì¥ì„ ì¡°ìœ¨í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
"""

import time
import logging
from typing import List, Dict
from dotenv import load_dotenv

# ë¶„ë¦¬ëœ ëª¨ë“ˆë“¤ import (ë™ì¼ íŒ¨í‚¤ì§€ ë‚´ ìƒëŒ€ ê²½ë¡œ)
from .dcinside.list_scraper import get_post_list
from .dcinside.detail_scraper import get_post_detail, cleanup_old_images
from .manager.save_db import save_posts

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def crawl_gallery(pages: int = 1, delay: float = 2.0, save_to_db: bool = True, max_posts: int = None, cleanup_days: int = 7) -> List[Dict]:
    """
    ê°¤ëŸ¬ë¦¬ í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜
    
    Args:
        pages: í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜
        delay: í˜ì´ì§€ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        save_to_db: MongoDB ì €ì¥ ì—¬ë¶€
        max_posts: ìµœëŒ€ í¬ë¡¤ë§ ê²Œì‹œê¸€ ìˆ˜ (Noneì´ë©´ ì œí•œ ì—†ìŒ)
        cleanup_days: ì´ë¯¸ì§€ ë³´ê´€ ê¸°ê°„ (ì¼). 0ì´ë©´ ì •ë¦¬ ì•ˆ í•¨
    
    Returns:
        í¬ë¡¤ë§í•œ ì „ì²´ ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸
    """
    logger.info(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘: {pages}í˜ì´ì§€, ì§€ì—° {delay}ì´ˆ" + (f", ìµœëŒ€ {max_posts}ê°œ" if max_posts else ""))
    
    # ì˜¤ë˜ëœ ì´ë¯¸ì§€ ìë™ ì •ë¦¬
    if cleanup_days > 0:
        cleanup_old_images(keep_days=cleanup_days)
    
    all_posts = []
    
    # 1ë‹¨ê³„: ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘
    for page in range(1, pages + 1):
        posts = get_post_list(page=page, recommend_only=True)
        
        # 2ë‹¨ê³„: ê° ê²Œì‹œê¸€ ë³¸ë¬¸ ìˆ˜ì§‘
        for post in posts:
            # ìµœëŒ€ ê²Œì‹œê¸€ ìˆ˜ ì²´í¬
            if max_posts and len(all_posts) >= max_posts:
                logger.info(f"â¹ï¸ ìµœëŒ€ ê²Œì‹œê¸€ ìˆ˜({max_posts}ê°œ) ë„ë‹¬, í¬ë¡¤ë§ ì¤‘ë‹¨")
                break
            
            time.sleep(delay)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
            
            detail = get_post_detail(post['post_id'])
            if detail:
                # ëª©ë¡ ì •ë³´ì™€ ë³¸ë¬¸ ì •ë³´ ë³‘í•©
                merged = {**post, **detail}
                all_posts.append(merged)
        
        # ìµœëŒ€ ê²Œì‹œê¸€ ìˆ˜ ë„ë‹¬ ì‹œ í˜ì´ì§€ ë£¨í”„ë„ ì¤‘ë‹¨
        if max_posts and len(all_posts) >= max_posts:
            break
        
        time.sleep(delay)  # í˜ì´ì§€ ê°„ ëŒ€ê¸°
    
    logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ê²Œì‹œê¸€")
    
    # MongoDB ì €ì¥
    if save_to_db:
        save_posts(all_posts)
    
    return all_posts


if __name__ == '__main__':
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    crawl_gallery(pages=2, delay=1.5)
